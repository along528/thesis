
%might try to show that fake estimate can be approximated just
%by TTL inputs, maybe using a histogram
%they are produced under ~/analysis/www/scripts/plot/plotDataMxM_Comparisons.py


As discussed in \sec\ref{sec:object_selection},
the leptons reconstructed by the ATLAS detector are selected
to optimize the measurement resolution and identification efficiency.
But this identification is not perfect. A jet,
for instance, perhaps from a charged pion, could leave a single
track in the inner detector along with a narrow energy deposit in the
EM calorimeter; a very similar signature to an electron. Or, 
a \bee-hadron could decay into a final state with a high energy muon,
making it difficult to distinguish from a muon produced in the hard interaction.
We call these mis-reconstructed leptons, ``fake'' leptons. 
By contrast, those leptons that have been correctly identified are 
referred to as ``real''.

In particle physics it is 
never the case that the features describing 
a given particle are completely separable
from another, even hypothetically. Instead,
the characteristic features for a type of particle will 
overlap with that of other particles. For example, both electrons
and jets are characterized in part by the presence of calorimeter
deposits in the EM calorimeter. 
The calorimeter deposits form a cone 
pointing back to the collision point, and the radius
of this cone will follow some distribution. On average,
the deposit from an electron will have a smaller radius 
than that of a jet. So, on average the radius of calorimeter
deposits can be used to distinguish between the two physics
processes. But the overlap of these two distributions is 
significant enough that using this radius alone
will give an unsatisfactorily high error rate for identification.
The error rate can be improved by adding information 
from the inner detector, and so on, further reducing
the error rate but never reaching zero.
So, while rare, the large number of collisions produced by the LHC
means that the measurement of fake leptons will inevitably occur. 
Thus, we must take them into account.

The modeling of these fake leptons are in general 
heavily dependent upon the conditions of the detector. 
The detector is described in MC simulation using \geant, thus
it is possible and relatively straightforward 
to model these processes using MC directly.
However, in practice, this usually proves to be inadequate
because some of the effects which produce fake leptons 
are so rare that it may be difficult to generate enough MC
collisions to obtain adequate statistics.
The dataset from the LHC, however, has an extremely large sample size.
The trick is then how to extract from the data the information we need
for the signal regions of interest in an accurate and unbiased way.


We choose to use the Generalized Matrix Method~\cite{Gillam:2014xua}, 
which may estimate from data the contribution of any combination
fake and real leptons. It has been implemented previously in 
\cite{Arguin:1558979}. Versions of the matrix
method have been implemented in previous experiments prior to the LHC.
The first version to be implemented within ATLAS
\cite{ATLAS-CONF-2010-087}
was restricted to the estimation of events with 
exactly one fake lepton.
Variations of the method have been implemented in numerous publications
by ATLAS and CMS ever since. % (though less often by CMS).
In essence, the method relies on the definition of two different selections,
referred to as ``tight'' and ``loose'', defined such that
``real'' leptons are more likely to pass the ``tight'' selection
than ``fake'' leptons. If the probability of the ``real'' and 
``fake'' leptons to pass these selections can be determined (typically
in control regions), 
then in principle the easily defined
``tight'' and ``loose'' selections may be used
as a proxy
to extract an estimate of the 
``real'' and ``fake'' lepton
contributions in a region of one's choosing.
The method is described in more detail below.



\subsubsection{Generalized Matrix Method}
\label{sec:mxm}

The Generalized Matrix Method allows one to extract from data the expected
number of events with any combination of fake and real leptons.
For any given selection, some fraction of the events will have 
real leptons, fake leptons, or some combination of the two.
For a selection with exactly one lepton, the lepton 
can simply be either real or fake.  Suppose one then defines
two orthogonal single lepton
selections with in general different combinations of real and fake leptons.
Furthermore, design one of the selections to be much more likely to have
real leptons than fake leptons, usually taken 
to be the signal region selection. We will call this the ``tight'' selection.
We can measure directly the number of events in the data that
pass this ``tight'' selection and call it $n_T$. Choose the other selection
to have a different composition of real and fake leptons. Since the 
``tight'' selection is enriched in real leptons, this can be achieved
if this other selection has a larger proportion of fake leptons.
We will call this the
``loose'' selection and designate the number of events measured
in this selection as $n_L$. 

The total number of real leptons that fall in 
both regions can be called $n_R$. The probability that one of these
real leptons passes the tight selection is called the 
real efficiency, or sometimes the real rate, and is denoted by 
$\real$. Similarly, the total number of fake leptons 
that fall in both regions is denoted $n_F$ and the probability 
that one of these fake leptons passes the tight selection is 
called the fake efficiency, or fake rate, and is 
denoted by $\fake$. The condition that
more real leptons pass the tight selection can thus be summarized by saying
that $\real>>\fake$ be true.

The expected values
%\footnote{The issue of deriving an expected value instead from a 
%measured value is an important one which shows how 
%this method can be break down as will be discussed shortly.} 
of  $n_T$ and $n_L$,
denoted $\langle n_T\rangle$ and $\langle n_L\rangle$,
can be related to $n_R$ and $n_F$ using these rates via a system of
equations:
\begin{align}
  \label{eq:mxm_single}
  \begin{pmatrix} \langle n_T\rangle \\ \langle n_L\rangle \end{pmatrix} 
  &= 
  \begin{pmatrix}
  \real & \fake \\ \realbar & \fakebar
  \end{pmatrix} 
  \begin{pmatrix} n_R \\ n_F \end{pmatrix}
\end{align}
where I have introduced the notation $\realbar = 1-\real$
and $\fakebar =1-\fake$.
Note that this equation is a function of the measured values of
$n_R$ and $n_F$ which we are actually seeking to find in terms of the 
expectations of $n_T$ and $n_L$. Thus, it is in fact more
useful to solve for $n_R$ and $n_F$ by taking the inverse:
\begin{equation}
  \label{eq:mxm_single_inverted}
  \begin{pmatrix} n_R \\ n_F \end{pmatrix} 
  =
  %&= 
  \frac{1}{\real-\fake}
  \begin{pmatrix}
  \fakebar & -\fake \\ -\realbar & \real
  \end{pmatrix} 
  \begin{pmatrix} \langle n_T\rangle \\ \langle n_L\rangle \end{pmatrix}
\end{equation}

So far everything is exact and as long as the condition
that $\real>>\fake$ is true,
as it should be by construction, 
then there is no risk of encountering the singular condition
when $\real=\fake$.
But in the matrix method, 
we wish to use the \emph{measured} values of $n_T$
and $n_T$ to derive an \emph{estimate} of the expectation
for $n_R$ and $n_F$, denoted $\hat{n}_R$ and $\hat{n}_F$. 
Thus, in a rather \emph{ad hoc} way 
we interpret \eqn\eqref{eq:mxm_single_inverted} as follows:
\begin{equation}
  \label{eq:mxm_single_inverted_approx}
  \begin{pmatrix} \langle n_R\rangle \\ \langle n_F\rangle \end{pmatrix} 
  %&
  \approx
  \begin{pmatrix} \hat{n}_R \\ \hat{n}_F \end{pmatrix} 
  %&= 
  =
  \frac{1}{\real-\fake}
  \begin{pmatrix}
  \fakebar & -\fake \\ -\realbar & \real
  \end{pmatrix} 
  \begin{pmatrix} n_T \\ n_L \end{pmatrix}
\end{equation}
This equation solves for the estimators, $\hat{n}_R$ and $\hat{n}_F$,
as a function of the measured values $n_T$ and $n_L$, as well 
as the rates. The estimators are in general only approximately equal
to the expected values, as discussed in \cite{Gillam:2014xua}.
This approximation can break down, sometimes even giving negative values
for the estimate.  Though it should be adequate if the number
of events falling in the ``tight'' and ``loose'' selections are 
not too small.
We will assume that the method holds, but these concerns are important
to keep in mind whenever using this method.


We now have a way to approximately solve for the estimate
of the real and fake lepton contributions to a single lepton 
selection in our data sample. But, ultimately we are interested
in an estimate of 
the number of fake leptons that fall into our tight selection, call it
estimate $\hat{f}_{T}$.
And in principle we can also solve for the number of fake leptons
that are loose, $\hat{f}_{L}$, though this is not our focus. 
These estimates can be solved for then in a straightforward way, by 
selecting only the estimated component of fakes.
\begin{equation}
  \begin{pmatrix} \hat{f}_T \\ \hat{f}_L \end{pmatrix} 
  %&= 
  =
  \begin{pmatrix}
  \real & \fake \\ \realbar & \fakebar
  \end{pmatrix} 
  \begin{pmatrix} 0\\ \hat{n}_F \end{pmatrix}
  %&=
  =
  \begin{pmatrix}
  \real & \fake \\ \realbar & \fakebar
  \end{pmatrix} 
  \begin{pmatrix}
  0 & 0\\ 0 & 1
  \end{pmatrix} 
  \begin{pmatrix} \hat{n}_R\\ \hat{n}_F \end{pmatrix}
\end{equation}
Solving for $\hat{n}_R$ and $\hat{n}_F$ and then 
Substituting in for equation \eqn\eqref{eq:mxm_single_inverted_approx}
gives an expression for the expected number of tight and loose
selected fake leptons as determined from the rates and the measured
value of tight and loose leptons:
\begin{align}
  \label{eq:mxm_final_single}
  \begin{pmatrix} \hat{f}_T \\ \hat{f}_L \end{pmatrix} 
  &=
  \begin{pmatrix}
  \real & \fake \\ \realbar & \fakebar
  \end{pmatrix} 
  \begin{pmatrix}
  0 & 0\\ 0 & 1
  \end{pmatrix} 
  \frac{1}{\real-\fake}
  \begin{pmatrix}
  \fakebar & -\fake \\ -\realbar& \real
  \end{pmatrix} 
  \begin{pmatrix} n_T\\ n_L \end{pmatrix}
\end{align}
Then,  since we are only interested in $\hat{f}_T$, 
we may simply pluck out the estimated number of tight leptons
from fakes:
\begin{align}
  \label{eq:mxm_final_single2}
  \begin{pmatrix} \hat{f}_T \\ 0 \end{pmatrix} 
  &=
  \begin{pmatrix}
  1 & 0\\ 0 & 0
  \end{pmatrix} 
  \begin{pmatrix}
  \real & \fake \\ \realbar & \fakebar
  \end{pmatrix} 
  \begin{pmatrix}
  0 & 0\\ 0 & 1
  \end{pmatrix} 
  \frac{1}{\real-\fake}
  \begin{pmatrix}
  \fakebar & -\fake \\ -\realbar& \real
  \end{pmatrix} 
  \begin{pmatrix} n_T\\ n_L \end{pmatrix}
\end{align}
Evaluating the expression for $\hat{f}_T$ gives:
\begin{align}
\hat{f}_T &= \frac{\fake}{\real-\fake} \Big( \real(n_T+n_L) - n_T \Big)  \\
           &= \Big( \frac{\fake}{\real-\fake} 
	      -\real\Big) n_T 
	      + \Big(\frac{\fake}{\real-\fake} \real \Big)n_L\\
           &= w_T~n_T + w_L~n_L
\end{align}
where in the last line we have reorganized the coefficients in front
of $n_T$ and $n_L$ into parameters $w_T$ and $w_L$ which are dependent
upon the rates. 

Practically, the final estimate of $\hat{f}_T$
can be determined by looping over each event in data, weighting each event
using either $w_T$ for those passing the tight selection and
$w_L$ for those passing the loose selection, and then 
summing up all of the weighted events.  This is a very useful 
strategy since it allows one to compute the estimate on the fly
using a setup similar to the one already used to process the data itself. %reword?
Note that since $\real>>\fake$ and $0<\real<1$,
$w_T$ will always be negative. Thus the method will produce negative weights.
This is not a concern as long as we keep in mind that the sum
is the only thing that is ultimately of interest.
However, it is worth noticing that the total estimate can itself
be negative when 
%$\real n_L< \realbar n_T$.
$\real / \realbar < n_T/n_L$.
Though this can in general be avoided as long as $\real$
is close to unity and if $n_L$ is
as large as or larger than $n_T$, which should usually be the case
anyway. In any case, it shows that it is possible to get negative
results if the proper conditions are not met.

It will prove useful to rewrite \eqn\eqref{eq:mxm_final_single}
in a more general form:
\begin{equation}
\label{eq:mxm_general}
\hat{F} = \mathbf{\Phi}\mathbf{W}\mathbf{\Phi}^{-1} N
\end{equation}
where for the single lepton case
\begin{equation}
N=\begin{pmatrix} n_T \\ n_L\end{pmatrix}
\end{equation}
and 
\begin{equation}
\hat{F}=\begin{pmatrix} \hat{f}_T \\ \hat{f}_L\end{pmatrix}.
\end{equation}
The quantity $\mathbf{\Phi}$ is the matrix from \eqn\eqref{eq:mxm_single}
\begin{equation}
\mathbf{\Phi} = 
  \begin{pmatrix}
  \real& \fake\\ \realbar& \fakebar
  \end{pmatrix} 
\end{equation}
and $\mathbf{\Phi}^{-1}$ is its inverse. Finally, $\mathbf{W}$
is the fake selection matrix which in this case is identified with
\begin{equation}
\mathbf{W}=\begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix}
\end{equation}
And if we want only the estimate of the remaining tight leptons
like in \eqn\eqref{eq:mxm_final_single2}
then we can do 
\begin{equation}
\label{eq:mxm_general_tight}
\hat{T} = \mathbf{M}\mathbf{\Phi}\mathbf{W}\mathbf{\Phi}^{-1} N
\end{equation}
where 
\begin{equation}
\hat{T}=\begin{pmatrix} \hat{f}_T \\ 0 \end{pmatrix}
\end{equation}
and $\mathbf{M}$ is the tight selection matrix:
\begin{equation}
\mathbf{M}=\begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}
\end{equation}




So far we have considered only the rates
in a single category or bin and for a single lepton. 
But this process can be extended easily for different bins with in general different rates, 
(using the lepton \pt, for instance)
by simply keeping track of each bin using an index. For example,
in bin $i$, one would measure the rates $\real^i$ and $\fake^i$
as well as the values $n_T^i$ and $n_L^i$ to arrive at the expectations
for $\hat{f}^i_T$ and $\hat{f}^i_L$ in bin $i$.
Equation~\eqref{eq:mxm_general} then becomes 
$\hat{F}^i = \mathbf{\Phi}^i\mathbf{W}(\mathbf{\Phi}^{-1})^i N^i$.
One may then sum over all the of the bins to get a total estimate
if desired.

The matrix method can be also extended to multiple leptons,
resulting in the generalized matrix method. 
Consider the three lepton case, which is most relevant to this
analysis. 
Equation \eqref{eq:mxm_general_tight} becomes
\begin{equation}
\hat{T}^{ijk} = \mathbf{M}\mathbf{\Phi}^{ijk}\mathbf{W}(\mathbf{\Phi}^{-1})^{ijk} N^{ijk}
\end{equation}
where each of the three leptons can be in separate bins $i$, $j$, and $k$.
The matrix $\mathbf{\Phi}^{ijk}$
can by constructed by taking the Kronecker product, denoted by $\otimes$,
of the individual single lepton matrices of rates for each lepton:
\begin{align}
  \mathbf{\Phi}^{ijk} &=
  \begin{pmatrix}
  \real^i & \fake^i \\ \realbar^i & \fakebar^i
  \end{pmatrix} 
  \otimes
  \begin{pmatrix}
  \real^j & \fake^j \\ \realbar^j & \fakebar^j
  \end{pmatrix} 
  \otimes
  \begin{pmatrix}
  \real^k & \fake^k \\ \realbar^k & \fakebar^k
  \end{pmatrix} \\
  &=
  \begin{pmatrix} 
  \real^i\real^j\real^k  &
  \real^i\real^j\fake^k  &
  \real^i\fake^j\real^k  &
  \real^i\fake^j\fake^k  &
  \fake^i\real^j\real^k  &
  \fake^i\real^j\fake^k  &
  \fake^i\fake^j\real^k  &
  \fake^i\fake^j\fake^k  \\
  \real^i\real^j\realbar^k  &
  \real^i\real^j\fakebar^k  &
  \real^i\fake^j\realbar^k  &
  \real^i\fake^j\fakebar^k  &
  \fake^i\real^j\realbar^k  &
  \fake^i\real^j\fakebar^k  &
  \fake^i\fake^j\realbar^k  &
  \fake^i\fake^j\fakebar^k  \\
  \real^i\realbar^j\real^k  &
  \real^i\realbar^j\fake^k  &
  \real^i\fakebar^j\real^k  &
  \real^i\fakebar^j\fake^k  &
  \fake^i\realbar^j\real^k  &
  \fake^i\realbar^j\fake^k  &
  \fake^i\fakebar^j\real^k  &
  \fake^i\fakebar^j\fake^k  \\
  \real^i\realbar^j\realbar^k  &
  \real^i\realbar^j\fakebar^k  &
  \real^i\fakebar^j\realbar^k  &
  \real^i\fakebar^j\fakebar^k  &
  \fake^i\realbar^j\realbar^k  &
  \fake^i\realbar^j\fakebar^k  &
  \fake^i\fakebar^j\realbar^k  &
  \fake^i\fakebar^j\fakebar^k  \\
  \realbar^i\real^j\real^k  &
  \realbar^i\real^j\fake^k  &
  \realbar^i\fake^j\real^k  &
  \realbar^i\fake^j\fake^k  &
  \fakebar^i\real^j\real^k  &
  \fakebar^i\real^j\fake^k  &
  \fakebar^i\fake^j\real^k  &
  \fakebar^i\fake^j\fakebar^k  \\
  \realbar^i\real^j\realbar^k  &
  \realbar^i\real^j\fakebar^k  &
  \realbar^i\fake^j\realbar^k  &
  \realbar^i\fake^j\fakebar^k  &
  \fakebar^i\real^j\realbar^k  &
  \fakebar^i\real^j\fakebar^k  &
  \fakebar^i\fake^j\realbar^k  &
  \fakebar^i\fake^j\fakebar^k  \\
  \realbar^i\realbar^j\real^k  &
  \realbar^i\realbar^j\fake^k  &
  \realbar^i\fakebar^j\real^k  &
  \realbar^i\fakebar^j\fake^k  &
  \fakebar^i\realbar^j\real^k  &
  \fakebar^i\realbar^j\fake^k  &
  \fakebar^i\fakebar^j\real^k  &
  \fakebar^i\fakebar^j\fake^k  \\
  \realbar^i\realbar^j\realbar^k  &
  \realbar^i\realbar^j\fakebar^k  &
  \realbar^i\fakebar^j\realbar^k  &
  \realbar^i\fakebar^j\fakebar^k  &
  \fakebar^i\realbar^j\realbar^k  &
  \fakebar^i\realbar^j\fakebar^k  &
  \fakebar^i\fakebar^j\realbar^k  &
  \fakebar^i\fakebar^j\fakebar^k  
  \end{pmatrix} 
\end{align}
and we can solve for the inverse.  
We are only interested in the components with at least one
fake lepton, thus we construct the matrix $\mathbf{W}$ such that:
\begin{equation}
\mathbf{W} = 
\begin{pmatrix}
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
\end{pmatrix}
\end{equation}
Furthermore, we have the vector 
\begin{equation}
N^{ijk}=\begin{pmatrix} 
  n_{TTT}^{ijk}\\
  n_{TTL}^{ijk}\\
  n_{TLT}^{ijk}\\
  n_{TLL}^{ijk}\\
  n_{LTT}^{ijk}\\
  n_{LTL}^{ijk}\\
  n_{LLT}^{ijk}\\
  n_{LLL}^{ijk}
  \end{pmatrix}.
\end{equation}
In this case, there is only one configuration that gives 
three tight leptons.  Thus, the matrix $\mathbf{M}$ is constructed
to be an $8 \times 8$ matrix with 1 in the first element and 
all other elements equal to 0. This results in the vector
$T^{ijk}$ having all elements equal to 0 except for the first,
denoted $\hat{f}_{TTT}^{ijk}$, which is the
estimate of the number of three lepton events with three tight leptons 
in bins $i$, $j$, and $k$,
where at least one lepton is fake.
Putting everything together, we can solve for 
$\hat{f}_{TTT}^{ijk}$:
\begin{align}
\begin{split}
\label{eq:mxm_threeleptons}
\hat{f}^{ijk}_{TTT} = ~& w_{TTT}(i,j,k)~n^{ijk}_{TTT}  \\
   &+(w_{TTL}(i,j,k)~n^{ijk}_{TTL} + j\leftrightarrow k + i \leftrightarrow k)\\
   &+(w_{LLT}(i,j,k)~n^{ijk}_{LLT} + j\leftrightarrow k + i \leftrightarrow k) \\
   &+w_{LLL}(i,k,j)~n^{ijk}_{LLL}
\end{split}
\end{align}
where the terms like $j\leftrightarrow k$ are intended to indicate
a copy of the first term in parentheses but with the indices 
switched as shown. Each term has a $w$ function that is a function
of the three lepton indices. These are the weights
extracted by the method and they end up taking a simple form:
\begin{equation}
\label{eq:mxm_wttt}
w_{TTT}(i,j,k) = 
-\frac{\real^i\fakebar^i}{\real^i-\fake^i}
\frac{\real^j\fakebar^j}{\real^j-\fake^j}
\frac{\real^k\fakebar^k}{\real^k-\fake^k}
\end{equation}
\begin{equation}
\label{eq:mxm_wttl}
w_{TTL}(i,j,k) = 
\frac{\real^i\fakebar^i}{\real^i-\fake^i}
\frac{\real^j\fakebar^j}{\real^j-\fake^j}
\frac{\real^k\fake^k}{\real^k-\fake^k}
\end{equation}
\begin{equation}
\label{eq:mxm_wllt}
w_{LLT}(i,j,k) =  
- \frac{\real^i\fake^i}{\real^i-\fake^i}
\frac{\real^j\fake^j}{\real^j-\fake^j}
\frac{\real^k\fakebar^k}{\real^k-\fake^k}
\end{equation}
\begin{equation}
\label{eq:mxm_wlll}
w_{LLL}(i,j,k) =  
\frac{\real^i\fake^i}{\real^i-\fake^i}
\frac{\real^j\fake^j}{\real^j-\fake^j}
\frac{\real^k\fake^k}{\real^k-\fake^k}
\end{equation}


One can see that for the case of zero or two loose leptons present, 
the magnitude of the weights are always negative (as long
as $\real > \fake$), while for those with one or three loose leptons 
present the magnitude is positive. As with the single
lepton case, this is not a concern as the sum should remain positive.
However, it might cause some concern to see that 
the magnitude of these weights decrease the more loose leptons 
are present, thus the highest magnitude weight
will in general be $w(i,j,k)_{TTT}$, which is negative!
Fortunately, in the sum this is balanced by the number of 
leptons observed, which tends to have the opposite trend.
As a result, it is those terms with exactly one loose lepton
observed that end up dominating the entire calculation, which
has a positive weight. 

\begin{table}[ht]
\centering
\begin{tabular}{c|cr@{}l}
Matrix Method Term & \multicolumn{3}{c}{Contribution} \\
\hline
\hline
TTT & &-217&.6\\
TTL & &3074&.8 \\
TLL & &-1&.2 \\
LLL & &2&.7 \\ 
Other & &0&.2 \\
\hline
Sum & &2858&.9
\end{tabular}
\label{tab:mxm_components}
\caption{Contribution of individual terms to the overall fake lepton 
prediction in the fake three lepton pre-selection region. The 
term called ``Other'' includes events with more than three leptons. }
\end{table}

The generalized matrix method has been 
evaluated using the rates described later 
in \sec\ref{sec:fake_and_real_rates}
at the pre-selection stage described in \sec\ref{sec:preselection}.
This is shown in \tab\ref{tab:mxm_components}
after summing over all bins $i$, $j$, and $k$.
The prediction is shown separately for each term in 
\eqn\eqref{eq:mxm_threeleptons}, where for example TTL means
the sum over the second line using the weights $w_{TTL}(i,j,k)$.
It also includes all other possible contributions from 
events with more than three leptons, labeled as ``Other''. 
From this it is clear that the TTL 
term (which also includes the TLT and LTT terms) 
dominates the calculation,
though the effects of the negative weights, in particular from the TTT
term, can clearly be seen in the sum. The contributions
from events with more than three leptons is observed to be small.
Thus, one could arrive at a good approximation to this full method
by just using \eqn\eqref{eq:mxm_threeleptons} along with just the
weights in \eqn\eqref{eq:mxm_wttt} and \eqref{eq:mxm_wttl}.

%\begin{figure}
%\centering
%\includegraphics[width=.45\columnwidth]{figures/CompareMxMComponents/ChargeSameSign_PreselCustomRates_Mar19/png/LeadingLeptonPt.png}
%\includegraphics[width=.45\columnwidth]{figures/CompareMxMComponents/ChargeSameSign_PreselCustomRates_Mar19/png/SubleadingLeptonPt.png}
%\includegraphics[width=.45\columnwidth]{figures/CompareMxMComponents/ChargeSameSign_PreselCustomRates_Mar19/png/MinimumLeptonPt.png}
%\includegraphics[width=.45\columnwidth]{figures/CompareMxMComponents/ChargeSameSign_PreselCustomRates_Mar19/png/MET_Et_STVF.png}
%\caption{Fake background estimate at pre-selection broken into tight and loose lepton configuration components for the leading (top left), sub-leading (top right), and minimum lepton \pt~(bottom left) as well as the \met~(bottom right). The three lepton TTT (orange dots), TTL (pink dots), TLL (blue dots), and LLL (yellow dots) components are shown along with any other components (green dots) such as those with four leptons initially.  The sum of all these components is also shown (black line). (what if I showed the absolute value of these weights?)}
%\label{fig:mxm_components}
%\end{figure}


In the analysis, a specialized code is used to evaluate the 
Generalized Matrix Method
on all possible combinations of input and output leptons and checks
to see which leptons pass the final selection. 
It uses the on-the-fly
weighting method described above and uses a tensor
formulation that improves the computational efficiency of the method.
This is also used as described in \cite{Gillam:2014xua}.
Uncertainties are calculated by propagating through 
the uncertainties on the rates. 
Using the standard propagation of uncertainty, this relies
on the derivative of the expectation with respect to the rates.
Fortunately, this can be calculated in a straightforward way,
though it will not be described here.
Correlations between different bins are assumed to be negligible and 
are ignored.  However, since the method relies on calculating multiple
weights from the same event, there is a correlation in the uncertainty
if these weights end up falling in the same bin. To handle this 
correlation the uncertainty for these weights are added linearly 
as opposed to in quadrature when extracting the final uncertainty 
on the method. The effect of treating the correlation on the uncertainty is observed to
be mostly negligible.

\subsubsection{Rate Determination}
\label{sec:fake_and_real_rates}

The Generalized Matrix Method relies on being able to determine
the real and fake rates to be used as inputs to the method.
This is usually done by looking into control regions
which are designed to be enhanced in sources of real and fake leptons.
It is important to note that we can never know with certainty 
whether a lepton is real or fake. Instead we must be clever enough
to find leptons that we are confident are of the appropriate type.
One clever trick is to use a method called the tag-and-probe method
to better identify real or fake leptons in the control regions;
it will be described shortly.
Once we have obtained our two separate collections of leptons,
one we believe to be rich in real leptons and the other in fake leptons,
we can use these leptons to extract the real and fake rates, respectively.
The real rate, $\real^i$, in category (or bin) $i$,
is simply defined as the ratio of tight candidate real leptons
over the number of tight plus loose candidate real leptons:
\begin{equation}
\label{eq:real_rate}
\real^i = \frac{r_T^i}{r_T^i+r_L^i}
\end{equation}
where $r_T^i$ and $r_L^i$ are the number of tight and loose candidate
real leptons in category $i$, to be 
distinguished from the $n_T^i$ and $n_L^i$ which
are the number of candidate and loose real leptons in the signal regions
and whose origin is unknown.
Similarly, the fake rate, $\fake^i$, in category $i$, 
is defined as the ratio of tight candidate fake leptons over
the number of tight plus loose candidate fake leptons:
\begin{equation}
\label{eq:fake_rate}
\fake^i = \frac{f_T^i}{f_T^i+f_L^i}
\end{equation}
where $f_T^i$ and $f_L^i$ are the number of tight and loose candidate
fake leptons in category $i$.

The real and fake rates are not universally the same for all leptons, 
and in fact can vary strongly. Thus, the choice of categories, $i$, is
an important one. The rates are usually split by lepton
flavor and also in bins of at least one kinematic quantity.
The splitting of the categories by flavor is 
very important as the rates are 
typically very different for electrons and muons. 
This is in part because
the loose and tight selections are usually chosen to be different
by necessity.
The tight selections are the same as in \sec\ref{sec:object_selection}
for both electrons and muons.  The loose selections, however,
are similar to the tight selection except that
the isolation requirements are removed and the object quality 
classification is loosened for electrons.
Another reason for categories by lepton flavor
is that the control regions which are enhanced in real and fake sources
of leptons are typically different for electrons and muons.
Thus, we choose to evaluate the rates separately for both.


The rates also tend to vary as a function of the lepton kinematics.
Thus, we further divide the electron
and muon categories into sub-categories of mutually exclusive bins of
$\pt$. The number of bins and the bin edges are determined 
to best capture the shape while also maintaining adequate statistics
in each category. In practice it is usually not possible to subdivide
the \pt~by more than a few bins. For the same reason, while
the rates also surely vary according to other kinematic criteria, 
like $\eta$, it is usually not possible to subdivide in more than
one kinematic variable and still have good statistics.


The control regions are chosen so as to be dominated by a single 
physics process.  For determining the real rates, the 
control region is chosen to be enhanced in $Z\rightarrow ll$
while the control region for determining the fake rates is
chosen to be enhanced in 
$W\rightarrow l\nu$ plus jets.
The reason for this choice is to allow for the application of the
tag-and-probe method, which uses one well defined lepton, the
``tag'', to identify the process, and another lepton, the ``probe'',
as the lepton under study. Both of these control regions
have at least one lepton object. 

%\begin{figure}[ht!]
%\centering
%\subfigure{
%\includegraphics[width=0.4\columnwidth]{figures/fakes_bkg/CRs/hPtElectronZBosonloosecut_total_new.eps}
%}
%\centering
%\subfigure{
%\includegraphics[width=0.4\columnwidth]{figures/fakes_bkg/CRs/hPtMuonZBosonloosecut_total_new.eps}
%}
%\vspace{-10mm}\caption{Invariant mass distribution of two opposite charge and same flavor di-lepton invariant mass electrons (left) and muons (right). Update figures!!}
%\label{fig:realEff_CRs}
%\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.495\columnwidth]{figures/fakes_bkg/CRs/RealTP/ProbeTightElectronPt_histratio.png}
\includegraphics[width=0.495\columnwidth]{figures/fakes_bkg/CRs/RealTP/ProbeTightMuonPt_histratio.png}
\includegraphics[width=0.495\columnwidth]{figures/fakes_bkg/CRs/RealTP/ProbeElectronPt_histratio.png}
\includegraphics[width=0.495\columnwidth]{figures/fakes_bkg/CRs/RealTP/ProbeMuonPt_histratio.png}
\caption{Probe lepton \pt\ distributions in SFOS tag and probe control regions used to derive real rates.  Electron (left) and muon (right) are shown
when the probe lepton is either tight (top) or no additional selection (besides the pre-selection) is required (bottom)}
\label{fig:realEff_CRsPt}
\end{figure}

In the control regions enhanced
in $Z\rightarrow ll$, if one well-reconstructed 
tag lepton passing the tight selection is found then the presence
of an additional lepton will almost certainly be the other real
lepton from the $Z$ decay. Thus, this second ``probe''
lepton, which can pass either the loose or tight selection
requirement is our candidate real lepton.
Note that if the probe lepton also passes the tight selection
then it could also be used as a tag. In fact, ignoring
this possibility can introduce a bias. Thus, we consider both 
leptons as either tag or probe candidates.
Only events where 
the tag lepton passes the same single lepton triggers and trigger
matching requirements as in \sec\ref{sec:event_selection} are used. 
We also require the presence of a 
probe lepton that forms an SFOS pair with the
tag whose di-lepton mass is within 10~\GeV of the \z-mass.
Two control regions are formed: one from $e^{+}e^{-}$ tag-probe
pairs for determining the electron real rates and another
from $\mu^{+}\mu^{-}$ tag-probe pairs for determining the muon
real rates. 
%The \z-peak in the di-lepton invariant mass distribution for the two control regions are shown in \fig\ref{fig:realEff_CRs} comparing the data to the model.
Since the rates are also determined as a function of the lepton
\pt, the lepton \pt~distributions are shown in 
\fig\ref{fig:realEff_CRsPt}
for the data as well as the expectation.
They are show separately for electrons
and muons and based on whether the probe leptons
pass just the tight selection 
(the top row of \fig\ref{fig:realEff_CRsPt})
or both the loose and tight selections
(the bottom row of \fig\ref{fig:realEff_CRsPt}).
The data clearly agrees well with the expectation, which is dominated
by the $Z\rightarrow ll$ process, as expected. The ratio of the 
candidate real leptons passing just the tight selection 
over those passing the loose and tight selections 
determines the real rate according to \eqn\eqref{eq:real_rate}.

The real rates are shown separately for electrons and muons 
in \fig\ref{fig:realEff} after adjusting to a coarser binning
to improve the statistics. 
It is interesting to note that the real rates
are uniformly lower for electrons than for muons, but both follow
the same trend of increasing as a function of the lepton \pt, 
and are relatively high even for the lowest value of 81\%.
The difference between the rates calculated 
either the data or the MC exclusively 
is taken as a systematic uncertainty on the nominal estimate
from the data. 
The rates and the systematic uncertainties
are summarized for electrons
in \tab\ref{tab:realEff_El} and for muons in \tab\ref{tab:realEff_Mu}.




\begin{figure}[ht!]
\centering
\includegraphics[width=0.495\columnwidth]{figures/fakes_bkg/Efficiencies/ElectronRealRates.png}
\includegraphics[width=0.495\columnwidth]{figures/fakes_bkg/Efficiencies/MuonRealRates.png}
\caption{Real lepton efficiency as a function of \pt\ and measured in data (red) and MC (blue) for electrons (left) and muons (right).}
\label{fig:realEff}
\end{figure}


%\tabcolsep=0.11cm
\begin{table}[ht!]
\centering
\input{tables/ElectronRealRates.tex}
\caption{Measured real efficiencies for electrons including statistical and systematic absolute uncertainties. 
Systematic is calculated by taking the difference
between the efficiencies measured in data and MC.  The efficiency measured in data is used as the nominal central value.
} 
\label{tab:realEff_El}
\end{table} 

%\tabcolsep=0.11cm
\begin{table}[ht!]
\centering
\input{tables/MuonRealRates.tex}
\caption{Measured real efficiencies for muons including statistical and systematic absolute uncertainties.
Systematic is calculated by taking the difference
between the efficiencies measured in data and MC.  The efficiency measured in data is used as the nominal central value.
} 
\label{tab:realEff_Mu}
\end{table} 


On the other hand, in the 
$W\rightarrow l\nu + \textrm{Jets}$ control region there
is only one real lepton being produced by the process.
If a well reconstructed tag lepton passing the tight selection
is found in this control 
region it is most likely coming from the $W$ decay.
In this case, if we measure a second ``probe'' lepton it is most likely
a jet faking a lepton. Thus, we have found a candidate fake lepton.
The control regions are formed by requiring the presence
of one tag lepton passing the tight selection
plus trigger requirements
of \sec\ref{sec:event_selection} with a $\pt>40\GeV$ and 
a probe lepton passing either the loose or tight selection.
The leptons are required to have the same sign, since
on average a fake lepton will have equal probably of
a positive or negative charge, while background processes
like $WW$,$\ttbar$, and $Z$ production produce opposite-sign lepton
pairs.  Only muons are used as tag leptons. 
The reason for this is that the chance of an electron passing tight selection
to be a jet fake is higher than that for muons. It is also 
possible for electrons to come from photon conversion  (PC)
while for muons this is very unlikely. Thus, using only muons as tag
leptons further reduces contamination from backgrounds in this control
region. The control region is then split based on whether the probe
lepton is an electron or a muon in order to determine the electron
and muon fake rates separately.
Events with additional leptons are thrown away.
To suppress contamination from multi-jet background processes
to the $W\rightarrow l\nu + \textrm{Jets}$ process, like QCD,
a cut of $\met>10\GeV$ is also applied.

The fake rate that is determined depends upon the source of fake leptons.
One way to assess this sensitivity is to consider the number of
\bee-jets present in the event. We consider two different
cases regarding the \bee-jet multiplicity: inclusive and exclusive.
The inclusive case makes no requirement on the number of \bee-jets
while the exclusive case asks that at least one \bee-jet is present.
These two different scenarios are ultimately compared in order to assess
a final systematic on the fake rate. The exclusive case
is used as the nominal estimate.


The processes contributing to the fake rate are known
to not be well modeled by MC, as discussed earlier in \sec\ref{sec:bg_fake}.
This is the primary reason for attempting to estimate the fake
lepton contribution from data in the first place. Thus,
we do not seek to describe the data using MC.
However, this control region is also not as pure with sources
of fake leptons as the real lepton control region is for real leptons,
because the neutrino in the $W\rightarrow l\nu$ control region
cannot be identified directly.
In particular, there is a significant contamination from processes
with real leptons, like $WW$, $\ttbar$, and $Z$ processes as well
as processes from photon conversion sources, even after the attempts
at reducing these backgrounds in the control region selection described above.
These backgrounds can be modeled using MC and so we attempt to subtract the
MC estimates of these backgrounds from the data before extracting the fake
rates. In effect, this means that the values of $f_{T}^{i}$ and
$f_{L}^i$ in \eqn\eqref{eq:fake_rate} are not taken directly from the
data but are instead corrected by the subtraction
\begin{equation}
\label{eq:fake_num_breakdown}
f_{T/L}^{i} = 
N^{\textrm{Data},i}_{T/L} 
- N^{\textrm{Real},i}_{T/L}
- N^{\textrm{PC},i}_{T/L}
\end{equation}
where $N^{\textrm{Data},i}_{T/L}$ is the number of tight or loose
probe leptons selected from data in bin $i$ of the fake lepton control region,
while $N^{\textrm{Real},i}_{T/L}$  and $N^{\textrm{PC},i}_{T/L}$
are the number of tight or loose probe leptons estimated from MC
to fall in bin $i$ of the fake lepton control region for real
lepton and photon conversion background sources, respectively.
The separate contributions to these terms are shown as a function
of the lepton \pt~for muons in \fig\ref{fig:fakeEff_CRs_muon} and for electrons
in \fig\ref{fig:fakeEff_CRs_electron}. They are split based on whether the lepton
passes just the tight selection or both the tight and loose selections
and also by the inclusive and exclusive \bee-jet multiplicity categories.
These are then used to calculate the fake rate as in \eqn\eqref{eq:fake_rate}.
A detailed breakdown of the numbers going into the fake rate calculation
are summarized in the exclusive \bee-jet multiplicity category for 
electrons in \tab\ref{tab:fakeCR_Yield_Electron_BJetGt0}
and for muons in \tab\ref{tab:fakeCR_Yield_Muon_BJetGt0}.


\begin{table}[ht!]
%\input{tables/YieldTable_BJetGt0Electron.tex}
\centering
\input{tables/YieldTable_BJetGt0Electron_Slim.tex}
\caption{Calculation of fake rate for electrons when $\nbjet> 0$. }
%in the different $p_{T}$ bins used in the final method.
%The observed data are is dominated by the fake contribution in this control region
%while the contributions from real leptons and leptons from photon 
%conversion are computed using MC and shown separately for the individual hard 
%processes as well as for the sum.  This is shown both when the probe
%lepton is tight (top) and when it is either loose or tight (bottom). The
%final rate is computed by taking the ratio of the tight probes in data
%over all probes after subtracting out the real and photon conversion parts.  
%In all cases, only statistical uncertainties are shown.  }
\label{tab:fakeCR_Yield_Electron_BJetGt0}
\end{table}

\begin{table}[ht!]
%\input{tables/YieldTable_BJetGt0Muon.tex}
\centering
\input{tables/YieldTable_BJetGt0Muon_Slim.tex}
\caption{Calculation of fake rate for muons when $\nbjet > 0$. }
%in the different $p_{T}$ bins used in the final method.
%The observed data are is dominated by the fake contribution in this control region
%while the contributions from real leptons and leptons from photon 
%conversion are computed using MC and shown separately for the individual hard 
%processes as well as for the sum.  This is shown both when the probe
%lepton is tight (top) and when it is either loose or tight (bottom). The
%final rate is computed by taking the ratio of the tight probes in data
%over all probes after subtracting out the real and photon conversion parts.  
%In all cases, only statistical uncertainties are shown.  }
\label{tab:fakeCR_Yield_Muon_BJetGt0}
\end{table}



\newpage
\begin{figure}[ht]
\centering
\subfigure{
\includegraphics[width=0.45\columnwidth]{figures/fakes_bkg/CRs/SameSignMuonMuon/NoStack/ProbeTightMuonPt.eps}
}
%\centering
%\subfigure{
%\includegraphics[width=0.3\columnwidth]{figures/fakes_bkg/CRs/SameSignMuonMuon/NoStack/ProbeTightMuonPtBJetEq0.eps}
%}
\centering
\subfigure{
\includegraphics[width=0.45\columnwidth]{figures/fakes_bkg/CRs/SameSignMuonMuon/NoStack/ProbeTightMuonPtBJetGt0.eps}
}
%\vspace{-1mm}
\centering
\subfigure{
\includegraphics[width=0.45\columnwidth]{figures/fakes_bkg/CRs/SameSignMuonMuon/NoStack/ProbeLooseORTightMuonPt.eps}
}
%\centering
%\subfigure{
%\includegraphics[width=0.45\columnwidth]{figures/fakes_bkg/CRs/SameSignMuonMuon/NoStack/ProbeLooseORTightMuonPtBJetEq0.eps}
%}
\centering
\subfigure{
\includegraphics[width=0.45\columnwidth]{figures/fakes_bkg/CRs/SameSignMuonMuon/NoStack/ProbeLooseORTightMuonPtBJetGt0.eps}
}
%\vspace{-10mm}
\caption{Transverse momentum distributions \pt\ of tight probe 
muons (top) and loose OR tight probe muons (bottom) passing signal 
selection criteria in the control Same-Sign $\mu-\mu$ control region 
without any additional requirement on $b$-jets in the event (left) and 
at least one $b$-jet (right). 
The amount observed in data (black points) corresponds 
to $N_T^{\textrm{Data},i}+N_L^{\textrm{Data},i}$ (bottom) and $N_T^{\textrm{Data},i}$ (top) following the notation
in \eqn\ref{eq:fake_num_breakdown}.
Meanwhile, the contribution determined in MC to come from 
real leptons (blue line) and from photon conversion (red line) are shown 
separately; they are not stacked. The real lepton contribution corresponds to 
$N_T^{\textrm{Real},i}+N_L^{\textrm{Real},i}$ (bottom) and $N_T^{\textrm{Real},i}$ 
(top) and the photon conversion 
contribution 
corresponds to $N_T^{\textrm{PC},i}+N_L^{\textrm{PC},i}$ (bottom) 
and $N_T^{\textrm{PC},i}$ (top) again using the notation 
in Eq.~\ref{eq:fake_rate}. 
The photon conversion is 
observed to be negligible for muons.  }
\label{fig:fakeEff_CRs_muon}
\end{figure}

\begin{figure}[ht]
\centering
\subfigure{
\includegraphics[width=0.45\columnwidth]{figures/fakes_bkg/CRs/SameSignElectronMuon/NoStack/ProbeTightElectronPt.eps}
}
%\centering
%\subfigure{
%\includegraphics[width=0.3\columnwidth]{figures/fakes_bkg/CRs/SameSignElectronMuon/NoStack/ProbeTightElectronPtBJetEq0.eps}
%}
\centering
\subfigure{
\includegraphics[width=0.45\columnwidth]{figures/fakes_bkg/CRs/SameSignElectronMuon/NoStack/ProbeTightElectronPtBJetGt0.eps}
}
\centering
\subfigure{
\includegraphics[width=0.45\columnwidth]{figures/fakes_bkg/CRs/SameSignElectronMuon/NoStack/ProbeLooseORTightElectronPt.eps}
}
%\centering
%\subfigure{
%\includegraphics[width=0.3\columnwidth]{figures/fakes_bkg/CRs/SameSignElectronMuon/NoStack/ProbeLooseORTightElectronPtBJetEq0.eps}
%}
\centering
\subfigure{
\includegraphics[width=0.45\columnwidth]{figures/fakes_bkg/CRs/SameSignElectronMuon/NoStack/ProbeLooseORTightElectronPtBJetGt0.eps}
}
%\vspace{-1mm}
%\vspace{-10mm}
\caption{Transverse momentum distributions \pt\ of tight probe 
electrons (top) and loose OR tight probe muons (bottom) passing signal 
selection criteria in the control Same-Sign $e-\mu$ control region 
without any additional requirement on $b$-jets in the event (left) and 
at least one $b$-jet (right). 
The amount observed in data (black points) corresponds 
to $N_T^{\textrm{Data},i}+N_L^{\textrm{Data},i}$ (bottom) and $N_T^{\textrm{Data},i}$ (top) following the notation
in \eqn\ref{eq:fake_num_breakdown}.
Meanwhile, the contribution determined in MC to come from 
real leptons (blue line) and from photon conversion (red line) are shown 
separately; they are not stacked. The real lepton contribution corresponds to 
$N_T^{\textrm{Real},i}+N_L^{\textrm{Real},i}$ (bottom) and $N_T^{\textrm{Real},i}$ 
(top) and the photon conversion 
contribution 
corresponds to $N_T^{\textrm{PC},i}+N_L^{\textrm{PC},i}$ (bottom) 
and $N_T^{\textrm{PC},i}$ (top) again using the notation 
in Eq.~\ref{eq:fake_rate}. }
\label{fig:fakeEff_CRs_electron}
\end{figure}


\subsubsection{Fake lepton background validation}

%A Monte Carlo closure test of the generalized matrix method is performed.  The fake rates are computed from MC samples in the di-lepton control regions defined in section~\ref{sec:fakecomposition}, and the method is then applied on the most important MC samples contributing to the event pre-selection: $Z$+jets and $t\bar{t}$.  The event pre-selection is used for this test, because the statistics available for the MC samples containing fake leptons in the signal region is too small to be able to draw any conclusion. Figure~\ref{fig:MCFakeRatesClosure}, show the MC fake rates obtained from the CR, while figure~\ref{fig:MCClosureCheckMatrixMethod} show the MC agreement with the MC events re-weighted using the generalized matrix method in the event pre-selection region, for the third-leading lepton $\pt$ and the $\met$ distribution. As it can be seen the shape agreement and the overall normalization are pretty good, showing that the matrix method is performing well.  

%\begin{figure}[ht!]
%\centering
%\includegraphics[width=0.42\columnwidth]{figures/ClosureCheck_MatrixMethod/ElFakeRates_MC_topZjets_new.pdf}
%\includegraphics[width=0.42\columnwidth]{figures/ClosureCheck_MatrixMethod/MuFakeRates_MC_topZjets_new.pdf}
%\caption{Distribution of the fake rates obtained from MC samples in the di-lepton control regions. The errors shown here are statistical only. These rates are used to performed a MC closure check of the global matrix method.}
%\label{fig:MCFakeRatesClosure}
%\end{figure}

%\begin{figure}[ht!]
%\centering
%\includegraphics[width=0.42\columnwidth]{figures/ClosureCheck_MatrixMethod/PtThirdLepSignal_TTT_total_new.pdf}
%\includegraphics[width=0.42\columnwidth]{figures/ClosureCheck_MatrixMethod/VR_PMET_lepTTT_total_new.pdf}
%\caption{Distributions of the third leading lepton $\pt$ and $\met$ in the event pre-selection region, for $Z$+jets and $t\bar{t}$, compared to events from these samples re-weighted using the global matrix method and the rates shown in Figure~\ref{fig:MCFakeRatesClosure}. Good agreement is observed}
%\label{fig:MCClosureCheckMatrixMethod}
%\end{figure}




The performance of the fake background estimate is tested in a control region
designed to be enhanced in this background while being orthogonal to the signal regions
described in \sec\ref{sec:signal_regions}.
The control region starts by using the event pre-selection region described 
in Section~\ref{sec:preselection}. To reduce contamination 
from the $WZ$ process, we require that there are 0 SFOS 
lepton pairs present in the event.
Finally, in order to enforce 
orthogonality with the signal regions
we require that $\nbjet \geq 1$. 
As such it is very close to the 0 SFOS signal region where we are most sensitive.

\begin{table}[ht!]
\centering
\input{tables/FakeCR_Yields.tex}
\caption{Expected and observed yields for the fake lepton control region. Only statistical
uncertainties are shown.}
\label{tab:FakeCR}
\end{table}




The total predicted events and observed data in this region are shown in \tab\ref{tab:FakeCR}.
The control region is clearly dominated by the fake lepton background, with $10.91 \pm 0.73\stat$ 
fake lepton events predicted out of a total of $14.72 \pm 0.73\stat$. Furthermore, the shapes
of the predicted and observed kinematic distributions are also shown along with the
fake lepton background systematic uncertainties in \fig\ref{fig:FakeCR}.  From this, 
we can see that the data is largely consistent with the prediction. This is true not just
for the overall normalization, but also for the shapes, though the control region is also 
limited by the number of statistics available. Since the fake lepton background
seems to describe the data well in this control region, we have confidence in the method
and choose to use the estimate also in the signal regions. 



\begin{figure}[h!]
\centering
\includegraphics[width=0.45\textwidth]{figures/Fake_CR/LeadingLeptonPt_histratio.eps}
\includegraphics[width=0.45\textwidth]{figures/Fake_CR/SubleadingLeptonPt_histratio.eps}
\includegraphics[width=0.45\textwidth]{figures/Fake_CR/MinimumLeptonPt_histratio.eps}
\includegraphics[width=0.45\textwidth]{figures/Fake_CR/MET_Et_histratio.eps}
\includegraphics[width=0.45\textwidth]{figures/Fake_CR/NBTaggedJets_histratio.eps}
\includegraphics[width=0.45\textwidth]{figures/Fake_CR/NJets_histratio.eps}
%\includegraphics[width=0.3\columnwidth]{figures/Fake_CR/NMuons_histratio.eps}
\caption{Distributions in a control region designed to study the data-driven 
fake lepton background estimate.  }
\label{fig:FakeCR}
\end{figure}






%\input{www_estimate_fake_oldpart}

